---
title: "Biomedicine: Mechanism Discovery"
description: "How causal pathfinding turns scattered biomedical evidence into auditable mechanistic chainsâ€”moving from correlation to explanation."
---

--8<-- "includes/quicknav.html"

# Biomedicine: Mechanism Discovery

<div class="landing-hero">
  <div class="landing-hero__grid">
    <div>
      <p class="landing-kicker">Case study â†’ biomedicine</p>
      <h2 class="landing-title">Mechanism discovery: from â€œrelevant papersâ€ to testable causal chains.</h2>
      <p class="landing-subtitle">
        The question is not whether two concepts co-occur in text.
        The question is whether there is a mechanistic chain you can inspect, challenge, and experimentally validate.
      </p>
      <div class="landing-cta">
        <a class="md-button md-button--primary" href="/services/start/">Start a Conversation</a>
        <a class="md-button" href="/methodology/causalgraphrag/">CausalGraphRAG</a>
        <a class="md-button" href="/methodology/constraints/">Constraints &amp; SHACL</a>
      </div>
    </div>
  </div>
</div>

## The question

<div class="landing-section">
  <div class="landing-card">
    <p>
      How do we uncover mechanistic chains (not just correlations) around targets like <strong>CA IX</strong> in tumor microenvironments?
    </p>
  </div>
</div>

<div class="landing-section">
    <img class="glightbox" src="/assets/img/br-008825.png"/>
</div>

## Why probabilistic search fails (even when it is â€œhonestâ€)

<div class="landing-section">
  <div class="landing-grid">
    <div class="landing-card">
      <h3>Retrieval returns relevance</h3>
      <p>â€œHere are papers about CA IXâ€ does not equal â€œhere is a chain that explains the outcome.â€</p>
    </div>
    <div class="landing-card">
      <h3>Text summaries blur mechanisms</h3>
      <p>Models can produce cautious language (â€œevidence is mixedâ€) without specifying what would falsify which link.</p>
    </div>
    <div class="landing-card">
      <h3>No trace = no lab plan</h3>
      <p>Without a structured path and citations per edge, you canâ€™t design targeted experiments.</p>
    </div>
  </div>
</div>

## What changes with causal traversal

<div class="landing-section">
  <div class="landing-callout">
    <p><strong>We encode entities, interactions, and provenance into a causal graph and run directed pathfinding.</strong></p>
    <p class="landing-mini">The output is a candidate mechanism with evidence per edge â€” or an abstention with missing data requirements.</p>
  </div>

```mermaid
flowchart TB
%% Styles (brModel Standard)
classDef i fill:#D3D3D3,stroke-width:0px,color:#000;
classDef p fill:#B3D9FF,stroke-width:0px,color:#000;
classDef r fill:#FFFFB3,stroke-width:0px,color:#000;
classDef o fill:#C1F0C1,stroke-width:0px,color:#000;
classDef s fill:#FFB3B3,stroke-width:0px,color:#000;

I_Q(["ğŸ¯ Causal question<br>(what mechanism explains Y?)"]):::i
P_G("ğŸ§  Build causal graph + provenance"):::p
P_Trv("ğŸ•¸ï¸ Directed pathfinding"):::p
G_Path{"Path found?"}:::s
R_Path(["ğŸ§¬ Candidate mechanism chain<br>(edge-level evidence)"]):::r
R_Trace(["ğŸ§¾ Trace package<br>(edges, citations, versions)"]):::r
O_Lab(["âœ… Test plan<br>(what would falsify which link)"]):::o
S_Abs(["ğŸ›‘ Abstain + missing evidence list"]):::i

I_Q --> P_G --> P_Trv --> G_Path
G_Path -->|"yes"| R_Path --> R_Trace --> O_Lab
G_Path -->|"no"| S_Abs --> R_Trace

%% Clickable nodes
click P_Trv "/methodology/causalgraphrag/" "CausalGraphRAG"
click P_G "/methodology/property-and-knowledge-graphs/" "Graphs"
```

<p>ğŸ§¬ The key shift is <strong>directed traversal</strong>: we build <strong>ğŸ§  causal memory</strong>, run <strong>ğŸ•¸ï¸ pathfinding</strong>, and explicitly decide whether a mechanistic chain exists. Either way, the system outputs a <strong>ğŸ§¾ trace package</strong> â€” so the result is falsifiable, not rhetorical.</p>

</div>

## Diagram: evidence and provenance per edge

<div class="landing-section">

```mermaid
flowchart TB
%% Styles (brModel Standard)
classDef i fill:#D3D3D3,stroke-width:0px,color:#000;
classDef p fill:#B3D9FF,stroke-width:0px,color:#000;
classDef r fill:#FFFFB3,stroke-width:0px,color:#000;
classDef o fill:#C1F0C1,stroke-width:0px,color:#000;
classDef s fill:#FFB3B3,stroke-width:0px,color:#000;

I_S(["ğŸ“„ Source<br>(paper / dataset)"]):::i
P_Ingest("ğŸ§¼ Ingest + fingerprint"):::p
R_Src(["ğŸ“ Source record<br>(versioned)"]):::r

P_Extract("ğŸ§¾ Extract claim"):::p
R_C(["ğŸ§¾ Claim object<br>(who said what, when)"]):::r
G_Q{"Quality sufficient?"}:::s
S_Down(["ğŸ›‘ Downweight / flag<br>(low quality)"]):::i

P_Map("ğŸ“ Map measurements + units"):::p
G_Meas{"Measurement aligned?"}:::s
S_Ask(["ğŸ›‘ Missing measurement details"]):::i

P_E("ğŸ”— Edge assertion"):::p
R_Edge(["ğŸ”— Edge object<br>(X â†’ Y, direction)"]):::r

P_Ev("ğŸ“ Attach evidence per edge"):::p
R_Ev(["ğŸ“ Evidence bundle<br>(citations, snippets, stats)"]):::r
G_Conf{"Confounders controlled?"}:::s

P_Path("ğŸ§­ Assemble path candidate"):::p
R_T(["ğŸ§¾ Trace object<br>(edges + evidence + assumptions)"]):::r
O_Out(["âœ… Reviewable hypothesis" ]):::o

I_S --> P_Ingest --> R_Src --> P_Extract --> R_C --> G_Q
G_Q -->|"no"| S_Down --> P_Map
G_Q -->|"yes"| P_Map

P_Map --> G_Meas
G_Meas -->|"no"| S_Ask --> R_T
G_Meas -->|"yes"| P_E --> R_Edge --> P_Ev --> R_Ev --> G_Conf

G_Conf -->|"no"| S_Down --> P_Path
G_Conf -->|"yes"| P_Path --> R_T --> O_Out
```

<p>ğŸ“ Each edge in the chain is backed by <strong>explicit evidence</strong>, not just a summary. The trace ties <strong>claims</strong> â†’ <strong>edges</strong> â†’ <strong>paths</strong> into an artifact you can challenge and iterate. <strong>Product:</strong> an edge-level <strong>evidence bundle</strong> plus a <strong>trace object</strong> that makes the hypothesis reviewable and falsifiable.</p>

</div>

## Diagram: falsification loop (how uncertainty becomes a lab plan)

<div class="landing-section">

```mermaid
flowchart TB
%% Styles (brModel Standard)
classDef i fill:#D3D3D3,stroke-width:0px,color:#000;
classDef p fill:#B3D9FF,stroke-width:0px,color:#000;
classDef r fill:#FFFFB3,stroke-width:0px,color:#000;
classDef o fill:#C1F0C1,stroke-width:0px,color:#000;
classDef s fill:#FFB3B3,stroke-width:0px,color:#000;

R_Path(["ğŸ§¬ Candidate mechanism"]):::r
P_Weak("ğŸ” Identify weakest link"):::p
G_Fals{"Falsifiable?"}:::s
R_Exp(["ğŸ§ª Minimal experiment<br>(most informative intervention)"]):::r
O_Upd(["âœ… Update graph + confidence"]):::o
S_Miss(["ğŸ›‘ Not falsifiable yet<br>request missing measurements"]):::i

R_Path --> P_Weak --> G_Fals
G_Fals -->|"yes"| R_Exp --> O_Upd --> R_Path
G_Fals -->|"no"| S_Miss --> R_Path
```

<p>ğŸ§ª This loop turns â€œmore readingâ€ into <strong>targeted falsification</strong>: find the weakest link, decide if itâ€™s falsifiable, run the smallest experiment that would flip your conclusion, then update the causal memory.</p>

</div>

## Outputs

<div class="landing-section">
  <div class="landing-grid">
    <div class="landing-card"><h3>Traceable paths</h3><p>Causal chains with supporting sources and versioned evidence.</p></div>
    <div class="landing-card"><h3>Hypotheses</h3><p>Candidates ranked by mechanistic plausibility, not by rhetorical fluency.</p></div>
    <div class="landing-card"><h3>Falsification plan</h3><p>Clear missing evidence and which link would change the conclusion.</p></div>
    <div class="landing-card"><h3>Iterability</h3><p>A model that improves as new studies arrive without losing auditability.</p></div>
    <div class="landing-card"><h3>Experiment prioritization</h3><p>Targeted follow-ups: which intervention would most reduce uncertainty in the mechanism chain.</p></div>
    <div class="landing-card"><h3>Evidence gaps map</h3><p>A structured view of missing links, contradictory studies, and where new data would unlock a valid path.</p></div>
  </div>
</div>

## Next steps

<div class="landing-section">
  <div class="landing-card">
    <p>
      <a class="md-button md-button--primary" href="/services/start/">Start a Conversation</a>
      <a class="md-button" href="/methodology/">Explore Methodology</a>
    </p>
  </div>
</div>
